{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pipeline-overview",
   "metadata": {},
   "source": [
    "# AutoGen Data Analysis Pipeline Overview\n",
    "\n",
    "This notebook provides an overview of the AutoGen Data Analysis Pipeline with AMP protocol integration.\n",
    "\n",
    "## Pipeline Architecture\n",
    "\n",
    "The pipeline consists of 6 specialized AutoGen agents:\n",
    "\n",
    "1. **Data Collector Agent** - Ingests data from various sources\n",
    "2. **Data Cleaner Agent** - Handles data quality and preprocessing\n",
    "3. **Statistical Analyst Agent** - Performs statistical analysis\n",
    "4. **ML Analyst Agent** - Builds and evaluates ML models\n",
    "5. **Visualization Agent** - Creates charts and dashboards\n",
    "6. **Quality Assurance Agent** - Validates results and ensures accuracy\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Multi-Agent Collaboration**: Agents work together using AutoGen's conversation framework\n",
    "- **AMP Protocol Integration**: Standardized communication between agents\n",
    "- **End-to-End Automation**: Complete data analysis workflow from ingestion to reporting\n",
    "- **Quality Assurance**: Built-in validation and quality checks\n",
    "- **Extensible Architecture**: Easy to add new agents and capabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add pipeline modules to path\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append(str(project_root / 'pipelines'))\n",
    "sys.path.append(str(project_root / '../shared-lib'))\n",
    "\n",
    "print(f\"Notebook directory: {notebook_dir}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline components\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pipelines.data_pipeline import DataAnalysisPipeline, PipelineConfig\n",
    "from amp_types import TransportType\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-config",
   "metadata": {},
   "source": [
    "## Pipeline Configuration\n",
    "\n",
    "Let's create a basic pipeline configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline configuration\n",
    "config = PipelineConfig(\n",
    "    pipeline_id=\"notebook_demo_001\",\n",
    "    pipeline_name=\"Notebook Demo Pipeline\",\n",
    "    \n",
    "    # Enable all components for demo\n",
    "    enable_data_collection=True,\n",
    "    enable_data_cleaning=True,\n",
    "    enable_statistical_analysis=True,\n",
    "    enable_ml_analysis=True,\n",
    "    enable_visualization=True,\n",
    "    enable_quality_assurance=True,\n",
    "    \n",
    "    # Quality settings\n",
    "    data_quality_threshold=0.8,\n",
    "    model_performance_threshold=0.7,\n",
    "    \n",
    "    # Output settings\n",
    "    generate_report=True,\n",
    "    create_dashboard=True,\n",
    "    save_artifacts=True,\n",
    "    \n",
    "    # LLM configuration (placeholder)\n",
    "    llm_config={\n",
    "        \"config_list\": [\n",
    "            {\n",
    "                \"model\": \"gpt-4\",\n",
    "                \"api_key\": os.environ.get(\"OPENAI_API_KEY\", \"demo-key\"),\n",
    "                \"api_type\": \"openai\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Pipeline ID: {config.pipeline_id}\")\n",
    "print(f\"Components enabled: {sum([config.enable_data_collection, config.enable_data_cleaning, config.enable_statistical_analysis, config.enable_ml_analysis, config.enable_visualization, config.enable_quality_assurance])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-data",
   "metadata": {},
   "source": [
    "## Sample Data\n",
    "\n",
    "Let's create some sample data for demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "\n",
    "# Generate synthetic employee data\n",
    "data = {\n",
    "    'employee_id': range(1, n_samples + 1),\n",
    "    'age': np.random.randint(22, 65, n_samples),\n",
    "    'department': np.random.choice(['Engineering', 'Sales', 'Marketing', 'HR'], n_samples),\n",
    "    'years_experience': np.random.randint(0, 25, n_samples),\n",
    "    'education_level': np.random.choice(['Bachelor', 'Master', 'PhD'], n_samples, p=[0.5, 0.35, 0.15]),\n",
    "    'salary': np.random.normal(75000, 20000, n_samples),\n",
    "    'satisfaction_score': np.random.uniform(1, 10, n_samples),\n",
    "    'performance_rating': np.random.choice(['Poor', 'Average', 'Good', 'Excellent'], n_samples, p=[0.1, 0.3, 0.4, 0.2])\n",
    "}\n",
    "\n",
    "# Add some correlations\n",
    "for i in range(n_samples):\n",
    "    # Salary correlates with experience and education\n",
    "    if data['education_level'][i] == 'PhD':\n",
    "        data['salary'][i] = max(data['salary'][i], np.random.normal(90000, 15000))\n",
    "    elif data['education_level'][i] == 'Master':\n",
    "        data['salary'][i] = max(data['salary'][i], np.random.normal(80000, 18000))\n",
    "    \n",
    "    data['salary'][i] += data['years_experience'][i] * 1200\n",
    "    \n",
    "    # Performance correlates with satisfaction\n",
    "    if data['satisfaction_score'][i] > 8:\n",
    "        data['performance_rating'][i] = np.random.choice(['Good', 'Excellent'], p=[0.3, 0.7])\n",
    "    elif data['satisfaction_score'][i] < 4:\n",
    "        data['performance_rating'][i] = np.random.choice(['Poor', 'Average'], p=[0.6, 0.4])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some missing values\n",
    "missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)\n",
    "df.loc[missing_indices, 'satisfaction_score'] = np.nan\n",
    "\n",
    "print(f\"Sample dataset created: {df.shape}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData Types:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nNumerical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Age distribution\n",
    "axes[0, 0].hist(df['age'], bins=20, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Age Distribution')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Salary by department\n",
    "df.boxplot(column='salary', by='department', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Salary by Department')\n",
    "axes[0, 1].set_xlabel('Department')\n",
    "axes[0, 1].set_ylabel('Salary')\n",
    "\n",
    "# Performance rating distribution\n",
    "performance_counts = df['performance_rating'].value_counts()\n",
    "axes[1, 0].bar(performance_counts.index, performance_counts.values)\n",
    "axes[1, 0].set_title('Performance Rating Distribution')\n",
    "axes[1, 0].set_xlabel('Performance Rating')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Satisfaction vs Salary scatter\n",
    "scatter = axes[1, 1].scatter(df['satisfaction_score'], df['salary'], alpha=0.6)\n",
    "axes[1, 1].set_title('Satisfaction vs Salary')\n",
    "axes[1, 1].set_xlabel('Satisfaction Score')\n",
    "axes[1, 1].set_ylabel('Salary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-demo",
   "metadata": {},
   "source": [
    "## Pipeline Demonstration\n",
    "\n",
    "**Note**: The following cells demonstrate the pipeline structure. In a real environment with proper API keys and AMP network setup, these would execute the full pipeline.\n",
    "\n",
    "For now, we'll show the pipeline initialization and structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = DataAnalysisPipeline(config)\n",
    "\n",
    "print(f\"Pipeline initialized: {pipeline.pipeline_id}\")\n",
    "print(f\"Agents available: {list(pipeline.agents.keys())}\")\n",
    "\n",
    "# Show agent capabilities\n",
    "for agent_name, agent in pipeline.agents.items():\n",
    "    print(f\"\\n{agent_name.upper()}:\")\n",
    "    print(f\"  Capabilities: {list(agent.capabilities.keys())}\")\n",
    "    print(f\"  Framework: {agent.amp_config.framework}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-sample-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sample data for pipeline processing\n",
    "sample_file = project_root / \"data\" / \"notebook_sample_data.csv\"\n",
    "sample_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(sample_file, index=False)\n",
    "\n",
    "print(f\"Sample data saved to: {sample_file}\")\n",
    "print(f\"Ready for pipeline processing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analysis-request",
   "metadata": {},
   "source": [
    "## Analysis Request\n",
    "\n",
    "Define what we want the pipeline to analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_request = \"\"\"\n",
    "Analyze the employee dataset to understand:\n",
    "1. Relationship between age, education, experience and salary\n",
    "2. Performance patterns across different departments\n",
    "3. Factors that predict employee satisfaction\n",
    "4. Build a classification model to predict performance rating\n",
    "5. Identify any data quality issues and recommendations\n",
    "\"\"\"\n",
    "\n",
    "context = {\n",
    "    \"business_context\": \"Employee performance and satisfaction analysis\",\n",
    "    \"target_metric\": \"performance_rating\",\n",
    "    \"key_stakeholders\": [\"HR\", \"Management\"],\n",
    "    \"analysis_type\": \"classification\"\n",
    "}\n",
    "\n",
    "print(\"Analysis Request:\")\n",
    "print(analysis_request)\n",
    "print(f\"\\nContext: {context}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-execution",
   "metadata": {},
   "source": [
    "## Pipeline Execution\n",
    "\n",
    "**Note**: The following cell would run the complete pipeline in a properly configured environment. \n",
    "\n",
    "```python\n",
    "# This would run the complete pipeline\n",
    "results = await pipeline.run_pipeline(\n",
    "    data_source=str(sample_file),\n",
    "    analysis_request=analysis_request,\n",
    "    context=context\n",
    ")\n",
    "```\n",
    "\n",
    "The pipeline would execute these steps:\n",
    "\n",
    "1. **Data Collection**: Load and validate the CSV file\n",
    "2. **Data Cleaning**: Handle missing values, detect outliers, remove duplicates\n",
    "3. **Statistical Analysis**: Descriptive statistics, correlation analysis, hypothesis testing\n",
    "4. **ML Analysis**: Feature engineering, model training (Random Forest, Logistic Regression, etc.)\n",
    "5. **Visualization**: Create plots, dashboards, and reports\n",
    "6. **Quality Assurance**: Validate data quality and model performance\n",
    "\n",
    "Each agent would contribute its specialized analysis and the results would be integrated into a comprehensive report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-outputs",
   "metadata": {},
   "source": [
    "## Expected Pipeline Outputs\n",
    "\n",
    "The pipeline would generate:\n",
    "\n",
    "### Data Collection Results\n",
    "- Dataset metadata (shape, columns, data types)\n",
    "- Data source validation report\n",
    "- Sample data preview\n",
    "\n",
    "### Data Cleaning Results\n",
    "- Missing value imputation report\n",
    "- Outlier detection and handling\n",
    "- Data quality score and recommendations\n",
    "\n",
    "### Statistical Analysis Results\n",
    "- Descriptive statistics for all variables\n",
    "- Correlation matrix and significant relationships\n",
    "- Hypothesis test results\n",
    "\n",
    "### ML Analysis Results\n",
    "- Feature importance ranking\n",
    "- Model performance comparison\n",
    "- Best model selection and metrics\n",
    "- Predictions and confidence intervals\n",
    "\n",
    "### Visualizations\n",
    "- Distribution plots\n",
    "- Correlation heatmaps\n",
    "- Model performance charts\n",
    "- Interactive dashboards\n",
    "\n",
    "### Quality Assurance Report\n",
    "- Data validation results\n",
    "- Model validation against thresholds\n",
    "- Pipeline audit and compliance check\n",
    "- Final recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To run this pipeline in a real environment:\n",
    "\n",
    "1. **Set up API Keys**: Configure OpenAI API key for LLM access\n",
    "2. **AMP Network**: Set up AMP protocol network (or use local mode)\n",
    "3. **Dependencies**: Install all required packages from requirements.txt\n",
    "4. **Configuration**: Adjust pipeline and agent configurations as needed\n",
    "5. **Execution**: Run the pipeline using the command line or this notebook\n",
    "\n",
    "### Command Line Usage\n",
    "\n",
    "```bash\n",
    "# Run with sample data\n",
    "python run_pipeline.py --sample\n",
    "\n",
    "# Run with custom data\n",
    "python run_pipeline.py --data data/notebook_sample_data.csv --request \"Predict employee performance\"\n",
    "```\n",
    "\n",
    "### Other Notebooks\n",
    "\n",
    "Explore other notebooks in this directory:\n",
    "- `02_Agent_Deep_Dive.ipynb` - Detailed exploration of individual agents\n",
    "- `03_Custom_Analysis.ipynb` - Custom analysis workflows\n",
    "- `04_Visualization_Examples.ipynb` - Visualization capabilities showcase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}