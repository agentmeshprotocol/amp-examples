{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "agent-deep-dive",
   "metadata": {},
   "source": [
    "# AutoGen Agent Deep Dive\n",
    "\n",
    "This notebook explores the individual agents in the data analysis pipeline, their capabilities, and how they work together.\n",
    "\n",
    "## Agent Architecture\n",
    "\n",
    "Each agent inherits from `AutoGenAMPAgent` which combines:\n",
    "- **AutoGen ConversableAgent**: Conversational AI capabilities\n",
    "- **AMP Protocol Integration**: Standardized communication\n",
    "- **Specialized Capabilities**: Domain-specific analysis functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project modules to path\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "sys.path.append(str(project_root))\n",
    "sys.path.append(str(project_root / 'agents'))\n",
    "sys.path.append(str(project_root / '../shared-lib'))\n",
    "\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import agent classes\n",
    "from agents.data_collector import DataCollectorAgent\n",
    "from agents.data_cleaner import DataCleanerAgent\n",
    "from agents.statistical_analyst import StatisticalAnalystAgent\n",
    "from agents.ml_analyst import MLAnalystAgent\n",
    "from agents.visualization_agent import VisualizationAgent\n",
    "from agents.quality_assurance import QualityAssuranceAgent\n",
    "\n",
    "from amp_client import AMPClientConfig\n",
    "from amp_types import TransportType\n",
    "\n",
    "print(\"Agent classes imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-configuration",
   "metadata": {},
   "source": [
    "## Agent Configuration\n",
    "\n",
    "Let's create configurations for each agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-configs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base LLM configuration\n",
    "llm_config = {\n",
    "    \"config_list\": [\n",
    "        {\n",
    "            \"model\": \"gpt-4\",\n",
    "            \"api_key\": os.environ.get(\"OPENAI_API_KEY\", \"demo-key\"),\n",
    "            \"api_type\": \"openai\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Base AMP configuration\n",
    "def create_amp_config(agent_type: str) -> AMPClientConfig:\n",
    "    return AMPClientConfig(\n",
    "        agent_id=f\"notebook_{agent_type}\",\n",
    "        agent_name=f\"Notebook {agent_type.title()}\",\n",
    "        framework=\"autogen\",\n",
    "        version=\"1.0.0\",\n",
    "        transport_type=TransportType.HTTP,\n",
    "        endpoint=\"http://localhost:8000\",\n",
    "        auto_reconnect=True,\n",
    "        log_level=\"INFO\"\n",
    "    )\n",
    "\n",
    "print(\"Configuration functions created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-collector-agent",
   "metadata": {},
   "source": [
    "## 1. Data Collector Agent\n",
    "\n",
    "Responsible for ingesting data from various sources:\n",
    "- File data (CSV, JSON, Excel, Parquet)\n",
    "- Database connections (SQLite, PostgreSQL, MySQL)\n",
    "- API endpoints (REST APIs)\n",
    "- Data source validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-collector-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Collector Agent\n",
    "data_collector = DataCollectorAgent(\n",
    "    amp_config=create_amp_config(\"data_collector\"),\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "print(f\"Data Collector Agent: {data_collector.name}\")\n",
    "print(f\"Capabilities: {list(data_collector.capabilities.keys())}\")\n",
    "print(f\"Description: {data_collector.description}\")\n",
    "print(f\"Tags: {data_collector.tags}\")\n",
    "\n",
    "# Show file readers\n",
    "print(f\"\\nSupported file types: {list(data_collector.file_readers.keys())}\")\n",
    "print(f\"Collection metrics: {data_collector.collection_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-collector-capabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Data Collector capabilities in detail\n",
    "for cap_id, capability in data_collector.capabilities.items():\n",
    "    print(f\"\\nCapability: {cap_id}\")\n",
    "    print(f\"  Description: {capability.description}\")\n",
    "    print(f\"  Category: {capability.category}\")\n",
    "    print(f\"  Input Schema: {list(capability.input_schema['properties'].keys())}\")\n",
    "    print(f\"  Response Time: {capability.constraints.response_time_ms}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-cleaner-agent",
   "metadata": {},
   "source": [
    "## 2. Data Cleaner Agent\n",
    "\n",
    "Handles data preprocessing and quality improvement:\n",
    "- Missing value detection and imputation\n",
    "- Outlier detection and handling\n",
    "- Data normalization and scaling\n",
    "- Duplicate removal\n",
    "- Data quality assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-cleaner-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Cleaner Agent\n",
    "data_cleaner = DataCleanerAgent(\n",
    "    amp_config=create_amp_config(\"data_cleaner\"),\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "print(f\"Data Cleaner Agent: {data_cleaner.name}\")\n",
    "print(f\"Capabilities: {list(data_cleaner.capabilities.keys())}\")\n",
    "print(f\"Description: {data_cleaner.description}\")\n",
    "print(f\"\\nCleaning configuration: {data_cleaner.cleaning_config}\")\n",
    "print(f\"Cleaning metrics: {data_cleaner.cleaning_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-analyst-agent",
   "metadata": {},
   "source": [
    "## 3. Statistical Analyst Agent\n",
    "\n",
    "Performs statistical analysis and hypothesis testing:\n",
    "- Descriptive statistics and distributions\n",
    "- Hypothesis testing (t-tests, ANOVA, chi-square)\n",
    "- Correlation and association analysis\n",
    "- Regression analysis with diagnostics\n",
    "- Distribution testing and normality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-analyst-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Statistical Analyst Agent\n",
    "statistical_analyst = StatisticalAnalystAgent(\n",
    "    amp_config=create_amp_config(\"statistical_analyst\"),\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "print(f\"Statistical Analyst Agent: {statistical_analyst.name}\")\n",
    "print(f\"Capabilities: {list(statistical_analyst.capabilities.keys())}\")\n",
    "print(f\"Description: {statistical_analyst.description}\")\n",
    "print(f\"\\nAnalysis configuration: {statistical_analyst.analysis_config}\")\n",
    "print(f\"Analysis metrics: {statistical_analyst.analysis_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml-analyst-agent",
   "metadata": {},
   "source": [
    "## 4. ML Analyst Agent\n",
    "\n",
    "Handles machine learning and predictive modeling:\n",
    "- Automated feature engineering and selection\n",
    "- Model training with multiple algorithms\n",
    "- Model evaluation and comparison\n",
    "- Predictions with confidence estimates\n",
    "- Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ml-analyst-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ML Analyst Agent\n",
    "ml_analyst = MLAnalystAgent(\n",
    "    amp_config=create_amp_config(\"ml_analyst\"),\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "print(f\"ML Analyst Agent: {ml_analyst.name}\")\n",
    "print(f\"Capabilities: {list(ml_analyst.capabilities.keys())}\")\n",
    "print(f\"Description: {ml_analyst.description}\")\n",
    "print(f\"\\nML configuration: {ml_analyst.ml_config}\")\n",
    "print(f\"\\nClassification algorithms: {list(ml_analyst.classification_algorithms.keys())}\")\n",
    "print(f\"Regression algorithms: {list(ml_analyst.regression_algorithms.keys())}\")\n",
    "print(f\"ML metrics: {ml_analyst.ml_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualization-agent",
   "metadata": {},
   "source": [
    "## 5. Visualization Agent\n",
    "\n",
    "Creates charts, dashboards, and reports:\n",
    "- Statistical plots (histograms, boxplots, scatter plots)\n",
    "- Model performance visualizations\n",
    "- Correlation and relationship plots\n",
    "- Interactive dashboards\n",
    "- Comprehensive analysis reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Visualization Agent\n",
    "visualization_agent = VisualizationAgent(\n",
    "    amp_config=create_amp_config(\"visualization\"),\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "print(f\"Visualization Agent: {visualization_agent.name}\")\n",
    "print(f\"Capabilities: {list(visualization_agent.capabilities.keys())}\")\n",
    "print(f\"Description: {visualization_agent.description}\")\n",
    "print(f\"\\nVisualization configuration: {visualization_agent.viz_config}\")\n",
    "print(f\"Visualization metrics: {visualization_agent.viz_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qa-agent",
   "metadata": {},
   "source": [
    "## 6. Quality Assurance Agent\n",
    "\n",
    "Validates results and ensures accuracy:\n",
    "- Data quality validation and integrity checks\n",
    "- Model performance validation and benchmarking\n",
    "- Statistical test validation and assumption checking\n",
    "- Pipeline audit and compliance checking\n",
    "- Result consistency verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qa-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Quality Assurance Agent\n",
    "qa_agent = QualityAssuranceAgent(\n",
    "    amp_config=create_amp_config(\"qa\"),\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "print(f\"QA Agent: {qa_agent.name}\")\n",
    "print(f\"Capabilities: {list(qa_agent.capabilities.keys())}\")\n",
    "print(f\"Description: {qa_agent.description}\")\n",
    "print(f\"\\nQA configuration: {qa_agent.qa_config}\")\n",
    "print(f\"QA standards: {qa_agent.qa_standards}\")\n",
    "print(f\"QA metrics: {qa_agent.qa_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-communication",
   "metadata": {},
   "source": [
    "## Agent Communication Patterns\n",
    "\n",
    "The agents communicate through:\n",
    "1. **AMP Protocol Messages**: Structured capability invocations\n",
    "2. **AutoGen Conversations**: Natural language discussions\n",
    "3. **Shared Artifacts**: Data and results passed between agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communication-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example conversation flow\n",
    "agents = {\n",
    "    \"Data Collector\": data_collector,\n",
    "    \"Data Cleaner\": data_cleaner,\n",
    "    \"Statistical Analyst\": statistical_analyst,\n",
    "    \"ML Analyst\": ml_analyst,\n",
    "    \"Visualization\": visualization_agent,\n",
    "    \"Quality Assurance\": qa_agent\n",
    "}\n",
    "\n",
    "print(\"Agent Communication Flow:\")\n",
    "print(\"1. Data Collector → Ingests data → Provides dataset\")\n",
    "print(\"2. Data Cleaner → Receives dataset → Cleans and validates → Provides clean dataset\")\n",
    "print(\"3. Statistical Analyst → Receives clean dataset → Performs analysis → Provides statistical insights\")\n",
    "print(\"4. ML Analyst → Receives clean dataset → Builds models → Provides ML results\")\n",
    "print(\"5. Visualization → Receives all results → Creates visualizations → Provides charts and reports\")\n",
    "print(\"6. Quality Assurance → Validates all outputs → Provides quality assessment\")\n",
    "\n",
    "print(\"\\nAMP Message Types:\")\n",
    "for agent_name, agent in agents.items():\n",
    "    print(f\"\\n{agent_name}:\")\n",
    "    for cap_id in agent.capabilities.keys():\n",
    "        print(f\"  - {cap_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conversation-examples",
   "metadata": {},
   "source": [
    "## Conversation Examples\n",
    "\n",
    "Here are examples of how agents would respond to natural language queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conversation-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate conversation responses\n",
    "conversation_examples = {\n",
    "    \"Data Collector\": {\n",
    "        \"query\": \"Can you collect data from a CSV file?\",\n",
    "        \"response\": data_collector._process_conversation_message(\n",
    "            \"collect data from file.csv\", None, []\n",
    "        )\n",
    "    },\n",
    "    \"Data Cleaner\": {\n",
    "        \"query\": \"How can you clean missing values?\",\n",
    "        \"response\": data_cleaner._process_conversation_message(\n",
    "            \"handle missing values\", None, []\n",
    "        )\n",
    "    },\n",
    "    \"Statistical Analyst\": {\n",
    "        \"query\": \"Can you perform correlation analysis?\",\n",
    "        \"response\": statistical_analyst._process_conversation_message(\n",
    "            \"correlation analysis\", None, []\n",
    "        )\n",
    "    },\n",
    "    \"ML Analyst\": {\n",
    "        \"query\": \"What machine learning models can you build?\",\n",
    "        \"response\": ml_analyst._process_conversation_message(\n",
    "            \"train model\", None, []\n",
    "        )\n",
    "    },\n",
    "    \"Visualization\": {\n",
    "        \"query\": \"Can you create a dashboard?\",\n",
    "        \"response\": visualization_agent._process_conversation_message(\n",
    "            \"dashboard\", None, []\n",
    "        )\n",
    "    },\n",
    "    \"Quality Assurance\": {\n",
    "        \"query\": \"How do you validate data quality?\",\n",
    "        \"response\": qa_agent._process_conversation_message(\n",
    "            \"validate data\", None, []\n",
    "        )\n",
    "    }\n",
    "}\n",
    "\n",
    "for agent_name, example in conversation_examples.items():\n",
    "    print(f\"\\n{agent_name.upper()}:\")\n",
    "    print(f\"Query: {example['query']}\")\n",
    "    print(f\"Response: {example['response'][:200]}...\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artifact-management",
   "metadata": {},
   "source": [
    "## Artifact Management\n",
    "\n",
    "Agents store and share data artifacts throughout the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artifact-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate artifact storage\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample data\n",
    "sample_data = pd.DataFrame({\n",
    "    'feature1': np.random.normal(0, 1, 100),\n",
    "    'feature2': np.random.normal(2, 1.5, 100),\n",
    "    'target': np.random.choice([0, 1], 100)\n",
    "})\n",
    "\n",
    "# Store artifact in data collector\n",
    "data_collector.store_artifact(\n",
    "    \"sample_dataset\", \n",
    "    sample_data, \n",
    "    {\"source\": \"synthetic\", \"rows\": 100, \"columns\": 3}\n",
    ")\n",
    "\n",
    "print(f\"Artifacts stored in Data Collector: {data_collector.list_artifacts()}\")\n",
    "\n",
    "# Retrieve artifact\n",
    "retrieved_data = data_collector.get_artifact(\"sample_dataset\")\n",
    "print(f\"Retrieved data shape: {retrieved_data.shape}\")\n",
    "print(f\"Retrieved data preview:\\n{retrieved_data.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-metrics",
   "metadata": {},
   "source": [
    "## Agent Performance Metrics\n",
    "\n",
    "Each agent tracks its performance and activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show agent metrics\n",
    "agent_metrics = {}\n",
    "\n",
    "for agent_name, agent in agents.items():\n",
    "    if hasattr(agent, 'collection_metrics'):\n",
    "        agent_metrics[agent_name] = agent.collection_metrics\n",
    "    elif hasattr(agent, 'cleaning_metrics'):\n",
    "        agent_metrics[agent_name] = agent.cleaning_metrics\n",
    "    elif hasattr(agent, 'analysis_metrics'):\n",
    "        agent_metrics[agent_name] = agent.analysis_metrics\n",
    "    elif hasattr(agent, 'ml_metrics'):\n",
    "        agent_metrics[agent_name] = agent.ml_metrics\n",
    "    elif hasattr(agent, 'viz_metrics'):\n",
    "        agent_metrics[agent_name] = agent.viz_metrics\n",
    "    elif hasattr(agent, 'qa_metrics'):\n",
    "        agent_metrics[agent_name] = agent.qa_metrics\n",
    "\n",
    "# Display metrics\n",
    "for agent_name, metrics in agent_metrics.items():\n",
    "    print(f\"\\n{agent_name} Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent-health",
   "metadata": {},
   "source": [
    "## Agent Health and Status\n",
    "\n",
    "Agents provide health check information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "health-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate health check (without actual AMP connection)\n",
    "for agent_name, agent in agents.items():\n",
    "    print(f\"\\n{agent_name} Status:\")\n",
    "    print(f\"  Agent ID: {agent.amp_config.agent_id}\")\n",
    "    print(f\"  Framework: {agent.amp_config.framework}\")\n",
    "    print(f\"  Capabilities: {len(agent.capabilities)}\")\n",
    "    print(f\"  Artifacts stored: {len(agent.list_artifacts())}\")\n",
    "    print(f\"  Context size: {len(agent._conversation_context)}\")\n",
    "    \n",
    "    # Show system message preview\n",
    "    system_msg = agent.system_message[:100] + \"...\" if len(agent.system_message) > 100 else agent.system_message\n",
    "    print(f\"  System message: {system_msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "customization",
   "metadata": {},
   "source": [
    "## Agent Customization\n",
    "\n",
    "Agents can be customized for specific use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "customization-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom ML Analyst with specific algorithms\n",
    "custom_ml_config = {\n",
    "    \"default_test_size\": 0.3,\n",
    "    \"cv_folds\": 10,\n",
    "    \"random_state\": 123,\n",
    "    \"max_features_auto\": 20\n",
    "}\n",
    "\n",
    "custom_ml_analyst = MLAnalystAgent(\n",
    "    amp_config=create_amp_config(\"custom_ml\"),\n",
    "    llm_config=llm_config,\n",
    "    ml_config=custom_ml_config\n",
    ")\n",
    "\n",
    "print(f\"Custom ML Analyst configuration: {custom_ml_analyst.ml_config}\")\n",
    "\n",
    "# Example: Custom Data Cleaner with specific strategies\n",
    "custom_cleaning_config = {\n",
    "    \"default_missing_strategy\": \"knn\",\n",
    "    \"outlier_threshold\": 2.5,\n",
    "    \"duplicate_threshold\": 0.99\n",
    "}\n",
    "\n",
    "custom_data_cleaner = DataCleanerAgent(\n",
    "    amp_config=create_amp_config(\"custom_cleaner\"),\n",
    "    llm_config=llm_config,\n",
    "    cleaning_config=custom_cleaning_config\n",
    ")\n",
    "\n",
    "print(f\"Custom Data Cleaner configuration: {custom_data_cleaner.cleaning_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps-agents",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Extending Agents\n",
    "\n",
    "You can extend agents by:\n",
    "1. **Adding new capabilities**: Implement new AMP capabilities\n",
    "2. **Customizing system messages**: Adjust agent personalities and instructions\n",
    "3. **Modifying configurations**: Change default parameters and thresholds\n",
    "4. **Adding domain logic**: Include domain-specific analysis methods\n",
    "\n",
    "### Creating New Agents\n",
    "\n",
    "To create a new agent:\n",
    "1. Inherit from `AutoGenAMPAgent`\n",
    "2. Define capabilities and their handlers\n",
    "3. Implement `_process_conversation_message`\n",
    "4. Add agent-specific configuration and metrics\n",
    "\n",
    "### Integration Patterns\n",
    "\n",
    "Agents can be integrated in different patterns:\n",
    "- **Sequential Pipeline**: Linear flow from collection to reporting\n",
    "- **Parallel Processing**: Multiple agents working simultaneously\n",
    "- **Iterative Refinement**: Agents providing feedback to improve results\n",
    "- **Conditional Workflows**: Different paths based on data characteristics\n",
    "\n",
    "Explore the other notebooks for more specific examples and use cases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}